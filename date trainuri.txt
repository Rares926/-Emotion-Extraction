_____________________________________________________________________________________________________

* model_name = "distilbert-base-uncased"
* num_labels = 7
* batch_size = 16
* results = "results/{}".format(model_name)
* lr = 2e-5
* num_epochs = 2 # [8,5,2]

results/distilbert-base-uncased/checkpoint-1282
Accuracy: 0.6305959086866291
              precision    recall  f1-score   support

           0       0.45      0.49      0.47       374
           1       0.00      0.00      0.00       111
           2       0.00      0.00      0.00       106
           3       0.61      0.58      0.59       594
           4       0.41      0.29      0.34       276
           5       0.65      0.65      0.65       447
           6       0.70      0.84      0.76      1465

    accuracy                           0.63      3373
   macro avg       0.40      0.41      0.40      3373
weighted avg       0.58      0.63      0.60      3373

_____________________________________________________________________________________________________

* model_name = "bert-base-uncased"
* num_labels = 7
* batch_size = 16
* results = "results/{}".format(model_name)
* lr = 2e-5
* num_epochs = 2

results/bert-base-uncased/checkpoint-6810

Accuracy: 0.6228876371182923
              precision    recall  f1-score   support

           0       0.41      0.56      0.47       374
           1       0.50      0.09      0.15       111
           2       0.35      0.07      0.11       106
           3       0.61      0.58      0.59       594
           4       0.41      0.33      0.37       276
           5       0.69      0.63      0.65       447
           6       0.71      0.79      0.75      1465

    accuracy                           0.62      3373
   macro avg       0.53      0.43      0.44      3373
weighted avg       0.61      0.62      0.61      3373

This one was trained on the whole context till the utterance.
But tested only on the utterance -> got better result 

Accuracy: 0.7592647494811741
              precision    recall  f1-score   support

           0       0.73      0.62      0.67       374
           1       0.73      0.34      0.47       111
           2       0.60      0.27      0.38       106
           3       0.78      0.71      0.74       594
           4       0.68      0.58      0.62       276
           5       0.76      0.76      0.76       447
           6       0.77      0.91      0.84      1465

    accuracy                           0.76      3373
   macro avg       0.72      0.60      0.64      3373
weighted avg       0.75      0.76      0.75      3373

results/bert-base-uncased_com_inp/checkpoint-2724

Accuracy: 0.6020558002936858
              precision    recall  f1-score   support

           0       0.51      0.45      0.48       346
           1       0.27      0.04      0.07        81
           2       0.09      0.03      0.05        61
           3       0.61      0.48      0.54       456
           4       0.43      0.32      0.36       263
           5       0.59      0.60      0.59       329
           6       0.65      0.82      0.73      1188

    accuracy                           0.60      2724
   macro avg       0.45      0.39      0.40      2724
weighted avg       0.57      0.60      0.58      2724
_____________________________________________________________________________________________________

* model_name = "roberta-base"
* num_labels = 7
* batch_size = 16
* results = "results/{}".format(model_name)
* lr = 2e-5
* num_epochs = 5 # [2,5]

results/roberta-base/checkpoint-1923

Accuracy: 0.6208123332345094
              precision    recall  f1-score   support

           0       0.52      0.40      0.45       374
           1       0.46      0.17      0.25       111
           2       0.23      0.11      0.15       106
           3       0.57      0.58      0.58       594
           4       0.41      0.39      0.40       276
           5       0.60      0.69      0.64       447
           6       0.72      0.79      0.75      1465

    accuracy                           0.62      3373
   macro avg       0.50      0.45      0.46      3373
weighted avg       0.60      0.62      0.61      3373
_____________________________________________________________________________________________________

* model_name = "bert-base-multilingual-uncased"
* num_labels = 7
* batch_size = 16
* results = "results/{}".format(model_name)
* lr = 2e-5
* num_epochs = 3 # [2,5]

results/bert-base-multilingual-uncased/checkpoint-1282

Accuracy: 0.6122146457159798
              precision    recall  f1-score   support

           0       0.41      0.58      0.48       374
           1       0.67      0.02      0.04       111
           2       0.00      0.00      0.00       106
           3       0.60      0.53      0.56       594
           4       0.42      0.23      0.30       276
           5       0.66      0.59      0.62       447
           6       0.69      0.82      0.75      1465

    accuracy                           0.61      3373
   macro avg       0.49      0.40      0.39      3373
weighted avg       0.59      0.61      0.58      3373

_____________________________________________________________________________________________________

* model_name = "xlnet/xlnet-base-cased"
* num_labels = 7
* batch_size = 16
* results = "results/{}".format(model_name)
* lr = 2e-5
* num_epochs = 3 # [2,5]

results/xlnet/xlnet-base-cased/checkpoint-1282

Accuracy: 0.6119181737325823
              precision    recall  f1-score   support

           0       0.40      0.56      0.47       374
           1       0.29      0.02      0.03       111
           2       0.00      0.00      0.00       106
           3       0.57      0.58      0.57       594
           4       0.42      0.26      0.33       276
           5       0.61      0.63      0.62       447
           6       0.72      0.79      0.75      1465

    accuracy                           0.61      3373
   macro avg       0.43      0.41      0.40      3373
weighted avg       0.58      0.61      0.59      3373

_____________________________________________________________________________________________________