{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine73/anaconda3/envs/MT_P/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import torch\n",
    "from datasets import Dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data such that a datapoint has the following\n",
    "* input is all the utterances from the text till that moment concatenated\n",
    "* label is the emotion of the last concatenated utterance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 13619\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = \"full_dataset.json\"\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "class_mapping = {\n",
    "            \"anger\":0,\n",
    "            \"disgust\":1,\n",
    "            \"fear\":2,\n",
    "            \"joy\":3,\n",
    "            \"sadness\":4,\n",
    "            \"surprise\":5,\n",
    "            \"neutral\":6\n",
    "}\n",
    "\n",
    "all_data = {\n",
    "    \"text\" : [],\n",
    "    \"label\" : []\n",
    "}\n",
    "\n",
    "for val in dataset:\n",
    "    concat_utt = \"\"\n",
    "    for utt in val[\"conversation\"]:\n",
    "        concat_utt = concat_utt + utt[\"text\"]\n",
    "        all_data[\"text\"].append(concat_utt)\n",
    "        all_data[\"label\"].append(class_mapping[utt[\"emotion\"]])\n",
    "\n",
    "created_dataset = Dataset.from_dict(all_data)\n",
    "created_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "num_labels = 7\n",
    "batch_size = 8\n",
    "results = \"results/{}\".format(model_name)\n",
    "lr = 2e-5\n",
    "num_epochs = 5 # [8,5,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13619/13619 [00:03<00:00, 4311.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 10895\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2724\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def prepare_for_train(tp_dat, split=True):\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "            )\n",
    "    tp_dat = tp_dat.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        batch_size=None)\n",
    "    \n",
    "    tp_dat.set_format(\n",
    "        \"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    if split:\n",
    "        tp_dat = tp_dat.train_test_split(\n",
    "            test_size=0.2,\n",
    "            shuffle = False,\n",
    "            seed = 1337\n",
    "            )\n",
    "        \n",
    "    return tp_dat\n",
    "\n",
    "train_test_data = prepare_for_train(created_dataset, True)\n",
    "\n",
    "train_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "logging_steps = len(train_test_data[\"train\"]) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(output_dir=results,\n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  learning_rate=lr,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1\",\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_test_data[\"train\"],\n",
    "                  eval_dataset=train_test_data[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 500/6810 [03:46<48:03,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3799, 'learning_rate': 1.853157121879589e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1000/6810 [07:35<44:16,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2035, 'learning_rate': 1.7063142437591777e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1362/6810 [10:20<39:53,  2.28it/s]\n",
      " 20%|██        | 1362/6810 [11:12<39:53,  2.28it/s]Checkpoint destination directory results/bert-base-uncased/checkpoint-1362 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2236084938049316, 'eval_accuracy': 0.5939794419970631, 'eval_f1': 0.5676638144701666, 'eval_runtime': 51.8583, 'eval_samples_per_second': 52.528, 'eval_steps_per_second': 6.576, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1500/6810 [12:31<40:15,  2.20it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1059, 'learning_rate': 1.5594713656387664e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2000/6810 [16:19<36:34,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9244, 'learning_rate': 1.4126284875183555e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2500/6810 [20:08<32:52,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9333, 'learning_rate': 1.2657856093979443e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2724/6810 [21:50<29:58,  2.27it/s]\n",
      " 40%|████      | 2724/6810 [22:42<29:58,  2.27it/s]Checkpoint destination directory results/bert-base-uncased/checkpoint-2724 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1991859674453735, 'eval_accuracy': 0.6020558002936858, 'eval_f1': 0.5789671747048933, 'eval_runtime': 51.9059, 'eval_samples_per_second': 52.48, 'eval_steps_per_second': 6.57, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 3000/6810 [25:04<29:02,  2.19it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7901, 'learning_rate': 1.1189427312775332e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 3500/6810 [28:52<25:07,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6501, 'learning_rate': 9.72099853157122e-06, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 4000/6810 [32:40<21:24,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.645, 'learning_rate': 8.252569750367108e-06, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 4086/6810 [33:19<19:55,  2.28it/s]\n",
      " 60%|██████    | 4086/6810 [34:11<19:55,  2.28it/s]Checkpoint destination directory results/bert-base-uncased/checkpoint-4086 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3886299133300781, 'eval_accuracy': 0.5796622613803231, 'eval_f1': 0.577820646482216, 'eval_runtime': 51.9165, 'eval_samples_per_second': 52.469, 'eval_steps_per_second': 6.568, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 4500/6810 [37:36<17:37,  2.19it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4661, 'learning_rate': 6.784140969162997e-06, 'epoch': 3.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 5000/6810 [41:25<13:47,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4329, 'learning_rate': 5.3157121879588845e-06, 'epoch': 3.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 5448/6810 [44:49<09:58,  2.28it/s]\n",
      " 80%|████████  | 5448/6810 [45:41<09:58,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6512377262115479, 'eval_accuracy': 0.5884728340675477, 'eval_f1': 0.574765816492121, 'eval_runtime': 51.8194, 'eval_samples_per_second': 52.567, 'eval_steps_per_second': 6.581, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 5500/6810 [46:17<09:54,  2.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4302, 'learning_rate': 3.847283406754773e-06, 'epoch': 4.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 6000/6810 [50:05<06:10,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2868, 'learning_rate': 2.378854625550661e-06, 'epoch': 4.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 6500/6810 [53:54<02:21,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3149, 'learning_rate': 9.104258443465493e-07, 'epoch': 4.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6810/6810 [56:16<00:00,  2.27it/s]\n",
      "100%|██████████| 6810/6810 [57:08<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8411591053009033, 'eval_accuracy': 0.5833333333333334, 'eval_f1': 0.575769359999327, 'eval_runtime': 51.8801, 'eval_samples_per_second': 52.506, 'eval_steps_per_second': 6.573, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6810/6810 [57:24<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3444.5086, 'train_samples_per_second': 15.815, 'train_steps_per_second': 1.977, 'train_loss': 0.7167956953006694, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6810, training_loss=0.7167956953006694, metrics={'train_runtime': 3444.5086, 'train_samples_per_second': 15.815, 'train_steps_per_second': 1.977, 'train_loss': 0.7167956953006694, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6020558002936858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.45      0.48       346\n",
      "           1       0.27      0.04      0.07        81\n",
      "           2       0.09      0.03      0.05        61\n",
      "           3       0.61      0.48      0.54       456\n",
      "           4       0.43      0.32      0.36       263\n",
      "           5       0.59      0.60      0.59       329\n",
      "           6       0.65      0.82      0.73      1188\n",
      "\n",
      "    accuracy                           0.60      2724\n",
      "   macro avg       0.45      0.39      0.40      2724\n",
      "weighted avg       0.57      0.60      0.58      2724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch \n",
    "\n",
    "model_name = \"results/bert-base-uncased_com_inp/checkpoint-2724\"\n",
    "num_labels = 7\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "eval_loader = DataLoader(train_test_data[\"test\"], batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"label\"]\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        # labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Store true and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "report = classification_report(true_labels, pred_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT_P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
