
\section*{Regressor}

1) Checked for best params from param param_grid

param_grid = {
    'solver': [ 'liblinear','saga'],
    'max_iter': [1000],
    'penalty': ['l1', 'l2', 'elasticnet']
}

Results:
Best parameters: {'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}
Best cross-validation score: 0.4843845778428501
              precision    recall  f1-score   support

       anger       0.26      0.09      0.13       374
     disgust       0.20      0.03      0.05       111
        fear       0.50      0.02      0.04       106
         joy       0.49      0.24      0.33       594
     neutral       0.50      0.88      0.64      1465
     sadness       0.42      0.16      0.23       276
    surprise       0.54      0.28      0.37       447

    accuracy                           0.49      3373
   macro avg       0.42      0.24      0.25      3373
weighted avg       0.46      0.49      0.42      3373

Accuracy: 0.48739994070560333

2) Checked for best params for C and multi_class

param_grid = {
    'C': np.logspace(-4, 4, 20),
    'multi_class':['ovr', 'multinomial']
}

Best parameters: {'C': 0.615848211066026, 'multi_class': 'ovr'}
Best cross-validation score: 0.48789834422502354
              precision    recall  f1-score   support

       anger       0.31      0.07      0.12       374
     disgust       0.20      0.02      0.03       111
        fear       0.50      0.01      0.02       106
         joy       0.51      0.23      0.31       594
     neutral       0.49      0.91      0.64      1465
     sadness       0.46      0.15      0.23       276
    surprise       0.55      0.26      0.35       447

    accuracy                           0.49      3373
   macro avg       0.43      0.24      0.24      3373
weighted avg       0.47      0.49      0.41      3373

Accuracy: 0.490364660539579

3)Results with best hyperparams

              precision    recall  f1-score   support

       anger       0.31      0.07      0.12       374
     disgust       0.25      0.02      0.03       111
        fear       1.00      0.01      0.02       106
         joy       0.52      0.23      0.31       594
     neutral       0.49      0.91      0.64      1465
     sadness       0.45      0.15      0.22       276
    surprise       0.55      0.26      0.35       447

    accuracy                           0.49      3373
   macro avg       0.51      0.23      0.24      3373
weighted avg       0.49      0.49      0.41      3373

Accuracy: 0.490364660539579
Cross-validated scores: [0.49596182 0.49375918 0.5101023 0.49082232 0.48806463]



4) MultinomialNB

Results
              precision    recall  f1-score   support

       anger       0.32      0.11      0.17       374
     disgust       0.40      0.02      0.03       111
        fear       0.00      0.00      0.00       106
         joy       0.50      0.23      0.31       594
     neutral       0.49      0.91      0.64      1465
     sadness       0.32      0.09      0.14       276
    surprise       0.59      0.26      0.36       447

    accuracy                           0.49      3373
   macro avg       0.38      0.23      0.24      3373
weighted avg       0.46      0.49      0.41      3373

Accuracy: 0.4879928846723985
Cross-validated scores: [0.46842878 0.48017621 0.47870778 0.47870778 0.47521116]
\section*{Fine-tuning}
\section*{Results}


Name                                Acc       F1       F1_weighted
MultinomialNB                       0.48      0.49     0.41
LogisticRegression-saga             0.50      0.48     0.45
LogisticRegression-lib-lin          0.49      0.49     0.41
distilbert-base-uncased             0.630              0.601
bert-base-uncased                   0.635              0.61
roberta-base                        0.622     0.62     0.61
bert-base-multilingual-uncased      0.61               0.58
xlnet/xlnet-base-cased              0.61      0.61     0.58
bert-base-uncased (CONVERSATION)    0.60      0.60     0.58