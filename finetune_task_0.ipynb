{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine73/anaconda3/envs/MT_P/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import csv\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_task_0(\n",
    "        json_path,\n",
    "        save_path,\n",
    "        test=False\n",
    "        ):\n",
    "\n",
    "    def process_dataset(example):\n",
    "        class_mapping = {\n",
    "            \"anger\":0,\n",
    "            \"disgust\":1,\n",
    "            \"fear\":2,\n",
    "            \"joy\":3,\n",
    "            \"sadness\":4,\n",
    "            \"surprise\":5,\n",
    "            \"neutral\":6\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"text\":example[\"text\"],\n",
    "            \"emotion\":example[\"emotion\"],\n",
    "            \"label\":class_mapping[example[\"emotion\"]]\n",
    "        }\n",
    "\n",
    "    with open(json_path, 'r') as file:\n",
    "        dataset = json.load(file)\n",
    "\n",
    "    all_test_data = []\n",
    "\n",
    "    for val in dataset[\"conversation\"].values():\n",
    "        all_test_data.extend([(v[\"text\"],v[\"emotion\"]) for v in val])\n",
    "\n",
    "    is_test = \"test\" if test else \"train\"\n",
    "\n",
    "    with open('{}/task_0_{}.csv'.format(save_path, is_test), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"text\",\"emotion\"])\n",
    "        writer.writerows(all_test_data)\n",
    "\n",
    "    raw_dataset = Dataset.from_csv('{}/task_0_{}.csv'.format(save_path, is_test), delimiter=\",\")\n",
    "\n",
    "    return raw_dataset.map(process_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3373 examples [00:00, 256665.23 examples/s]\n",
      "Map: 100%|██████████| 3373/3373 [00:00<00:00, 26969.65 examples/s]\n",
      "Generating train split: 10246 examples [00:00, 241033.57 examples/s]\n",
      "Map: 100%|██████████| 10246/10246 [00:00<00:00, 27759.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_task_0 = load_data_for_task_0(\n",
    "    json_path = 'test_dataset.json',\n",
    "    save_path=\"/media/sml0/RaresPatrascu/projects/Project2\",\n",
    "    test=True)\n",
    "\n",
    "train_data_task_0 = load_data_for_task_0(\n",
    "    json_path = 'train_dataset.json',\n",
    "    save_path=\"/media/sml0/RaresPatrascu/projects/Project2\",\n",
    "    test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dict class with both train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'emotion', 'label'],\n",
       "        num_rows: 10246\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'emotion', 'label'],\n",
       "        num_rows: 3373\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_task0 = DatasetDict({\n",
    "    'train': train_data_task_0,\n",
    "    'test': test_data_task_0\n",
    "})\n",
    "\n",
    "dataset_task0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "num_labels = 7\n",
    "batch_size = 16\n",
    "results = \"results/{}\".format(model_name)\n",
    "lr = 2e-5\n",
    "num_epochs = 2 # [8,5,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "num_labels = 7\n",
    "batch_size = 16\n",
    "results = \"results/{}\".format(model_name)\n",
    "lr = 2e-5\n",
    "num_epochs = 3 # [8,5,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "num_labels = 7\n",
    "batch_size = 16\n",
    "results = \"results/{}\".format(model_name)\n",
    "lr = 2e-5\n",
    "num_epochs = 5 # [2,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-uncased\"\n",
    "num_labels = 7\n",
    "batch_size = 16\n",
    "results = \"results/{}\".format(model_name)\n",
    "lr = 2e-5\n",
    "num_epochs = 3 # [2,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xlnet/xlnet-base-cased\"\n",
    "num_labels = 7\n",
    "batch_size = 16\n",
    "results = \"results/{}\".format(model_name)\n",
    "lr = 2e-5\n",
    "num_epochs = 3 # [2,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10246/10246 [00:00<00:00, 20143.25 examples/s]\n",
      "Map: 100%|██████████| 3373/3373 [00:00<00:00, 39562.27 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'emotion', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10246\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'emotion', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3373\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "        )\n",
    "\n",
    "task_0_data_encoded = dataset_task0.map(tokenize, batched=True, batch_size=None)\n",
    "task_0_data_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "task_0_data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create method to extract certain metrics dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1_weighted\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = len(task_0_data_encoded[\"train\"]) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(output_dir=results,\n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  learning_rate=lr,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1_weighted\",\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=task_0_data_encoded[\"train\"],\n",
    "                  eval_dataset=task_0_data_encoded[\"test\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup 1 train \n",
    "* model_name = \"distilbert-base-uncased\"\n",
    "* num_labels = 7\n",
    "* batch_size = 16\n",
    "* results = \"results/{}\".format(model_name)\n",
    "* lr = 2e-5\n",
    "* num_epochs = 2 # [8,5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 501/1282 [00:42<01:06, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.287, 'learning_rate': 1.2199687987519501e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|█████     | 641/1282 [00:57<00:50, 12.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1452741622924805, 'eval_accuracy': 0.6205158612511118, 'eval_f1': 0.5928145360130473, 'eval_runtime': 3.5332, 'eval_samples_per_second': 954.653, 'eval_steps_per_second': 59.719, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1002/1282 [01:35<00:23, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0571, 'learning_rate': 4.399375975039002e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1282/1282 [02:02<00:00, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1321868896484375, 'eval_accuracy': 0.6305959086866291, 'eval_f1': 0.601624597952231, 'eval_runtime': 3.5516, 'eval_samples_per_second': 949.701, 'eval_steps_per_second': 59.409, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1282/1282 [02:13<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 133.1271, 'train_samples_per_second': 153.928, 'train_steps_per_second': 9.63, 'train_loss': 1.1455190483009945, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1282, training_loss=1.1455190483009945, metrics={'train_runtime': 133.1271, 'train_samples_per_second': 153.928, 'train_steps_per_second': 9.63, 'train_loss': 1.1455190483009945, 'epoch': 2.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup 2 train \n",
    "\n",
    "* model_name = \"bert-base-uncased\"\n",
    "* num_labels = 7\n",
    "* batch_size = 16\n",
    "* results = \"results/{}\".format(model_name)\n",
    "* lr = 2e-5\n",
    "* num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 19%|█▉        | 371/1923 [04:13<04:22,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2581, 'learning_rate': 1.4799791991679668e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                  \n",
      "\u001b[A                                               \n",
      " 19%|█▉        | 371/1923 [04:44<04:22,  5.91it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1263704299926758, 'eval_accuracy': 0.6095463978654018, 'eval_f1_weighted': 0.586506661768034, 'eval_runtime': 7.2059, 'eval_samples_per_second': 468.087, 'eval_steps_per_second': 29.281, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 19%|█▉        | 371/1923 [05:58<04:22,  5.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9777, 'learning_rate': 9.599583983359335e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      " 19%|█▉        | 371/1923 [06:53<04:22,  5.91it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1270931959152222, 'eval_accuracy': 0.6335606285206048, 'eval_f1_weighted': 0.6097289428985562, 'eval_runtime': 7.2058, 'eval_samples_per_second': 468.094, 'eval_steps_per_second': 29.282, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 19%|█▉        | 371/1923 [07:42<04:22,  5.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8851, 'learning_rate': 4.399375975039002e-06, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      " 19%|█▉        | 371/1923 [09:02<04:22,  5.91it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.168025255203247, 'eval_accuracy': 0.6222946931514972, 'eval_f1_weighted': 0.6076702744186467, 'eval_runtime': 7.2352, 'eval_samples_per_second': 466.192, 'eval_steps_per_second': 29.163, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "100%|██████████| 1923/1923 [06:29<00:00,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 389.4732, 'train_samples_per_second': 78.922, 'train_steps_per_second': 4.937, 'train_loss': 0.9831831005173801, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1923, training_loss=0.9831831005173801, metrics={'train_runtime': 389.4732, 'train_samples_per_second': 78.922, 'train_steps_per_second': 4.937, 'train_loss': 0.9831831005173801, 'epoch': 3.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup 3 train \n",
    "\n",
    "* model_name = \"roberta-base\"\n",
    "* num_labels = 7\n",
    "* batch_size = 16\n",
    "* results = \"results/{}\".format(model_name)\n",
    "* lr = 2e-5\n",
    "* num_epochs = 5 # [2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3205 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 16%|█▌        | 501/3205 [01:24<07:37,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2872, 'learning_rate': 1.6879875195007804e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 640/3205 [01:47<07:13,  5.92it/s]\n",
      " 20%|██        | 641/3205 [01:54<07:12,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1466127634048462, 'eval_accuracy': 0.614882893566558, 'eval_f1': 0.5895575401927462, 'eval_runtime': 6.8337, 'eval_samples_per_second': 493.584, 'eval_steps_per_second': 30.876, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1001/3205 [03:09<06:16,  5.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0492, 'learning_rate': 1.3759750390015602e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 1281/3205 [03:56<05:25,  5.92it/s]\n",
      " 40%|████      | 1282/3205 [04:03<05:24,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1270663738250732, 'eval_accuracy': 0.6228876371182923, 'eval_f1': 0.6061412655820544, 'eval_runtime': 6.8621, 'eval_samples_per_second': 491.538, 'eval_steps_per_second': 30.748, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1501/3205 [04:54<04:48,  5.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9618, 'learning_rate': 1.06396255850234e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 1922/3205 [06:06<03:37,  5.90it/s]\n",
      " 60%|██████    | 1923/3205 [06:13<03:37,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1755762100219727, 'eval_accuracy': 0.6208123332345094, 'eval_f1': 0.6076601256607577, 'eval_runtime': 6.8461, 'eval_samples_per_second': 492.688, 'eval_steps_per_second': 30.82, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 2001/3205 [06:40<03:23,  5.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8411, 'learning_rate': 7.519500780031202e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2501/3205 [08:05<02:00,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7137, 'learning_rate': 4.399375975039002e-06, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 2563/3205 [08:15<01:48,  5.89it/s]\n",
      " 80%|████████  | 2564/3205 [08:22<01:48,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2405508756637573, 'eval_accuracy': 0.6181440853839312, 'eval_f1': 0.6059779485302749, 'eval_runtime': 6.9013, 'eval_samples_per_second': 488.747, 'eval_steps_per_second': 30.574, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 3001/3205 [09:50<00:34,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.612, 'learning_rate': 1.2792511700468018e-06, 'epoch': 4.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3204/3205 [10:25<00:00,  5.84it/s]\n",
      "100%|██████████| 3205/3205 [10:32<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2853273153305054, 'eval_accuracy': 0.6154758375333531, 'eval_f1': 0.6069171669696962, 'eval_runtime': 6.9024, 'eval_samples_per_second': 488.672, 'eval_steps_per_second': 30.569, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3205/3205 [10:50<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 650.7125, 'train_samples_per_second': 78.729, 'train_steps_per_second': 4.925, 'train_loss': 0.8916479411251645, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3205, training_loss=0.8916479411251645, metrics={'train_runtime': 650.7125, 'train_samples_per_second': 78.729, 'train_steps_per_second': 4.925, 'train_loss': 0.8916479411251645, 'epoch': 5.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup 4 train\n",
    "\n",
    "* model_name = \"bert-base-multilingual-uncased\"\n",
    "* num_labels = 7\n",
    "* batch_size = 16\n",
    "* results = \"results/{}\".format(model_name)\n",
    "* lr = 2e-5\n",
    "* num_epochs = 3 # [2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1923 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 26%|██▌       | 501/1923 [01:29<04:16,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3289, 'learning_rate': 1.4799791991679668e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 640/1923 [01:54<03:49,  5.58it/s]\n",
      " 33%|███▎      | 641/1923 [02:01<03:49,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1996480226516724, 'eval_accuracy': 0.597687518529499, 'eval_f1': 0.5629430787891094, 'eval_runtime': 6.9969, 'eval_samples_per_second': 482.07, 'eval_steps_per_second': 30.156, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1001/1923 [03:29<02:46,  5.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1063, 'learning_rate': 9.599583983359335e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1281/1923 [04:20<01:55,  5.57it/s]\n",
      " 67%|██████▋   | 1282/1923 [04:27<01:55,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.175890326499939, 'eval_accuracy': 0.6122146457159798, 'eval_f1': 0.5848778544402612, 'eval_runtime': 7.0298, 'eval_samples_per_second': 479.815, 'eval_steps_per_second': 30.015, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1501/1923 [05:29<01:16,  5.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0236, 'learning_rate': 4.399375975039002e-06, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1922/1923 [06:45<00:00,  5.55it/s]\n",
      "100%|██████████| 1923/1923 [06:52<00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2047953605651855, 'eval_accuracy': 0.6015416543136674, 'eval_f1': 0.5809296717149575, 'eval_runtime': 7.0141, 'eval_samples_per_second': 480.889, 'eval_steps_per_second': 30.082, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1923/1923 [07:21<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 441.5898, 'train_samples_per_second': 69.608, 'train_steps_per_second': 4.355, 'train_loss': 1.1028149902851583, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1923, training_loss=1.1028149902851583, metrics={'train_runtime': 441.5898, 'train_samples_per_second': 69.608, 'train_steps_per_second': 4.355, 'train_loss': 1.1028149902851583, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup 5 train \n",
    "\n",
    "* model_name = \"xlnet/xlnet-base-cased\"\n",
    "* num_labels = 7\n",
    "* batch_size = 16\n",
    "* results = \"results/{}\".format(model_name)\n",
    "* lr = 2e-5\n",
    "* num_epochs = 3 # [2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1923 [00:00<?, ?it/s]You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 26%|██▌       | 500/1923 [01:50<05:16,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.354, 'learning_rate': 1.4799791991679668e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 641/1923 [02:22<04:00,  5.32it/s]\n",
      " 33%|███▎      | 641/1923 [02:31<04:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.198628544807434, 'eval_accuracy': 0.6089534538986066, 'eval_f1': 0.5794785447548367, 'eval_runtime': 9.8245, 'eval_samples_per_second': 343.326, 'eval_steps_per_second': 21.477, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1001/1923 [04:05<03:25,  4.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1263, 'learning_rate': 9.599583983359335e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1282/1923 [05:07<01:59,  5.34it/s]\n",
      " 67%|██████▋   | 1282/1923 [05:17<01:59,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1720722913742065, 'eval_accuracy': 0.6119181737325823, 'eval_f1': 0.589517715230408, 'eval_runtime': 9.7961, 'eval_samples_per_second': 344.319, 'eval_steps_per_second': 21.539, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1501/1923 [06:18<01:34,  4.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.049, 'learning_rate': 4.399375975039002e-06, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1923/1923 [07:52<00:00,  5.36it/s]\n",
      "100%|██████████| 1923/1923 [08:02<00:00,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1730469465255737, 'eval_accuracy': 0.6101393418321969, 'eval_f1': 0.5898905375341711, 'eval_runtime': 9.8638, 'eval_samples_per_second': 341.957, 'eval_steps_per_second': 21.391, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1923/1923 [08:19<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 499.5051, 'train_samples_per_second': 61.537, 'train_steps_per_second': 3.85, 'train_loss': 1.1300495244907454, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1923, training_loss=1.1300495244907454, metrics={'train_runtime': 499.5051, 'train_samples_per_second': 61.537, 'train_steps_per_second': 3.85, 'train_loss': 1.1300495244907454, 'epoch': 3.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6228876371182923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.56      0.47       374\n",
      "           1       0.50      0.09      0.15       111\n",
      "           2       0.35      0.07      0.11       106\n",
      "           3       0.61      0.58      0.59       594\n",
      "           4       0.41      0.33      0.37       276\n",
      "           5       0.69      0.63      0.65       447\n",
      "           6       0.71      0.79      0.75      1465\n",
      "\n",
      "    accuracy                           0.62      3373\n",
      "   macro avg       0.53      0.43      0.44      3373\n",
      "weighted avg       0.61      0.62      0.61      3373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch \n",
    "\n",
    "model_name = \"results/bert-base-uncased_sim_inp/checkpoint-1282\"\n",
    "num_labels = 7\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "eval_loader = DataLoader(task_0_data_encoded[\"test\"], batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"label\"]\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        # labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Store true and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "report = classification_report(true_labels, pred_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT_P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
